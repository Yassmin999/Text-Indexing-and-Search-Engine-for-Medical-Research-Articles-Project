{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>DOCUMENTS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #file handling\n",
    "import json \n",
    "import xml.etree.ElementTree as ET #XML parsing\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "\n",
    "This line opens the file located at file_path in read mode ('r').\n",
    "The encoding='utf-8' argument ensures that the file is read using UTF-8 encoding, which is suitable for most text files, especially those containing non-ASCII characters.\n",
    "The with statement is used to ensure that the file is properly closed after its suite finishes, even if an error occurs. The variable f acts as a file object within this block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions for reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return {\"content\": file.read().split()}\n",
    "\n",
    "def read_xml(file_path):\n",
    "    try:\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Initialize a dictionary to store data\n",
    "        extracted_data = {}\n",
    "\n",
    "        # Iterate through all elements in the XML file\n",
    "        for elem in root.iter():\n",
    "            tag = elem.tag\n",
    "            text_content = elem.text.strip() if elem.text else \"\"\n",
    "\n",
    "            # Store non-empty content in the dictionary\n",
    "            if text_content:\n",
    "                if tag in extracted_data:\n",
    "                    extracted_data[tag].append(text_content)\n",
    "                else:\n",
    "                    extracted_data[tag] = [text_content]\n",
    "\n",
    "        return extracted_data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except ET.ParseError:\n",
    "        print(f\"Error parsing XML file: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "def read_html(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'html.parser')\n",
    "        \n",
    "        extracted_data = {}\n",
    "        \n",
    "        for tag in soup.find_all(True):  # True finds all tags\n",
    "            text_content = tag.get_text().strip()\n",
    "            if text_content:\n",
    "                if tag.name in extracted_data:\n",
    "                    extracted_data[tag.name].append(text_content)\n",
    "                else:\n",
    "                    extracted_data[tag.name] = [text_content]\n",
    "        \n",
    "        return extracted_data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing HTML file: {file_path}, Error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(file_path):\n",
    "    if file_path.endswith('.json'):\n",
    "        return read_json(file_path)\n",
    "    elif file_path.endswith('.txt'):\n",
    "        return read_txt(file_path)\n",
    "    elif file_path.endswith('.xml'):\n",
    "        return read_xml(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract text with keys for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_keys(data):\n",
    "    if isinstance(data, dict):\n",
    "        extracted = {}\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, (str, list, dict)):\n",
    "                # Extract text recursively and store under the respective key\n",
    "                extracted[key] = extract_text_with_keys(value)\n",
    "        # Convert the extracted dictionary into a string for concatenation\n",
    "        return ' '.join(f\"{k}: {v}\" for k, v in extracted.items())\n",
    "    elif isinstance(data, list):\n",
    "        # If it's a list, concatenate extracted text from each element as a single string.\n",
    "        return ' '.join(extract_text_with_keys(item) for item in data if isinstance(item, (str, dict, list)))\n",
    "    elif isinstance(data, str):\n",
    "        return data  # Base case: return the string directly\n",
    "    return ''  # Return an empty string for other types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand punctuated abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_punctuated_abbreviations(text):\n",
    "    punctuated_abbreviation_dict = {\n",
    "        \"aa.\": \"arteriae\",\n",
    "        \"vv.\": \"venae\",\n",
    "        \"v.\": \"vena\",\n",
    "        \"a.\": \"arteria\",\n",
    "        \"nn.\": \"nervi\",\n",
    "        \"n.\": \"nervus\",\n",
    "        \"mm.\": \"musculi\",\n",
    "        \"m.\": \"musculus\",\n",
    "        \"ligg.\": \"ligamenta\",\n",
    "        \"lig.\": \"ligamentum\",\n",
    "        \"procc.\": \"processus\",\n",
    "        \"proc.\": \"processus\",\n",
    "        \"art.\": \"articulatio\",\n",
    "        \"ggll.\": \"ganglia\",\n",
    "        \"ggl.\": \"ganglion\",\n",
    "        \"q.d.\": \"once a day\",\n",
    "        \"b.i.d.\": \"twice a day\",\n",
    "        \"t.i.d.\": \"three times a day\",\n",
    "        \"q.i.d.\": \"four times a day\",\n",
    "        \"q.o.d.\": \"every other day\",\n",
    "        \"a.c.\": \"before meals\",\n",
    "        \"p.c.\": \"after meals\",\n",
    "        \"p.r.n.\": \"as needed\",\n",
    "        \"p.o.\": \"by mouth\",\n",
    "        \"i.v.\": \"intravenous\",\n",
    "        \"i.m.\": \"intramuscular\",\n",
    "        \"s.c.\": \"subcutaneous\",\n",
    "        \"n.p.o.\": \"nothing by mouth\",\n",
    "        \"c.c.\": \"with food\",\n",
    "        \"a1.\": \"alpha 1\",\n",
    "        \"A.B.C.\": \"airway, breathing, circulation\",\n",
    "        \"etc.\": \"et cetera\"\n",
    "    }\n",
    "    \n",
    "    for abbrev, expansion in punctuated_abbreviation_dict.items():\n",
    "        text = text.replace(abbrev, expansion)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing sentences and words using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    # Ensure that the input is a string\n",
    "    if isinstance(text, dict):\n",
    "        text = ' '.join(f\"{k}: {' '.join(v) if isinstance(v, list) else v}\" for k, v in text.items())\n",
    "    elif isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = [word_tokenize(sentence) for sentence in sentences]\n",
    "    flat_words = [word for sentence in words for word in sentence]\n",
    "    return flat_words\n",
    "print(tokenize(\"Surgical ab- aaron's clipping or endovascular-coiling is often guided by preoperative assessment of the aa. communicans and aa. basilaris to ensure collateral circulation. The a. cerebri posterior, though less frequently affected, can also be a site of aneurysm formation in patients with hypertension or connective tissue disorders.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Understanding vascular variability is critical in managing cerebrovascular disorders. Variations in aa. communicans, aa. cerebri, and vv. cerebrales significantly impact clinical outcomes and treatment approaches. Advances in imaging technologies are enhancing the precision of diagnostic capabilities, aiding clinicians in the management of complex cerebrovascular conditions. Ongoing research on anatomical variations promises to further refine diagnostic protocols and therapeutic interventions in cerebrovascular medicine.\"\n",
    "expanded_text = expand_punctuated_abbreviations(text)\n",
    "tokens = tokenize(expanded_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate frequencies of each term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def calculate_word_frequency(tokens_per_doc):\n",
    "    word_frequencies = Counter()\n",
    "    for sentence_tokens in tokens_per_doc: \n",
    "        word_frequencies.update(sentence_tokens)\n",
    "    return word_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_terms_dict = {\n",
    "    \"flu\": 1000,\n",
    "    \"cyst\": 950,\n",
    "    \"tumor\": 900,\n",
    "    \"fever\": 890,\n",
    "    \"cancer\": 880,\n",
    "    \"pain\": 870,\n",
    "    \"stroke\": 850,\n",
    "    \"rash\": 840,\n",
    "    \"asthma\": 830,\n",
    "    \"diabetes\": 820,\n",
    "    \"anemia\": 800,\n",
    "    \"infection\": 780,\n",
    "    \"sepsis\": 760,\n",
    "    \"biopsy\": 750,\n",
    "    \"fracture\": 740,\n",
    "    \"allergy\": 730,\n",
    "    \"nausea\": 720,\n",
    "    \"fatigue\": 700,\n",
    "    \"migraine\": 680,\n",
    "    \"neoplasm\": 670,\n",
    "    \"arthritis\": 650,\n",
    "    \"chronic\": 640,\n",
    "    \"diagnosis\": 630,\n",
    "    \"therapy\": 620,\n",
    "    \"cardiology\": 600,\n",
    "    \"neurosis\": 590,\n",
    "    \"immunity\": 580,\n",
    "    \"concussion\": 570,\n",
    "    \"respiratory\": 560,\n",
    "    \"immunology\": 550,\n",
    "    \"hematology\": 540,\n",
    "    \"pathology\": 530,\n",
    "    \"urology\": 520,\n",
    "    \"radiology\": 510,\n",
    "    \"chemotherapy\": 500,\n",
    "    \"pharmacology\": 490,\n",
    "    \"epidemiology\": 480,\n",
    "    \"cardiogram\": 470,\n",
    "    \"endocrinology\": 460,\n",
    "    \"psychology\": 450,\n",
    "    \"hematoma\": 440,\n",
    "    \"intubation\": 430,\n",
    "    \"anaphylaxis\": 420,\n",
    "    \"psychiatry\": 410,\n",
    "    \"oncology\": 400,\n",
    "    \"fibromyalgia\": 390,\n",
    "    \"hypertension\": 380,\n",
    "    \"nephrology\": 370,\n",
    "    \"pediatrics\": 360,\n",
    "    \"ophthalmology\": 350,\n",
    "    \"osteoporosis\": 340,\n",
    "    \"gastroenteritis\": 330,\n",
    "    \"rheumatology\": 320,\n",
    "    \"thrombosis\": 310,\n",
    "    \"anesthesiology\": 300,\n",
    "    \"pulmonology\": 290,\n",
    "    \"electrocardiogram\": 280,\n",
    "    \"electroencephalogram\": 270,\n",
    "    \"histocompatibility\": 260,\n",
    "    \"otorhinolaryngology\": 250,\n",
    "    \"gastroenterology\": 240,\n",
    "    \"angiocardiography\": 230,\n",
    "    \"biopsychosocial\": 220,\n",
    "    \"immunodeficiency\": 210,\n",
    "    \"neuroendocrinology\": 200,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_medical_terms_with_real_frequencies(medical_dict, word_frequencies):\n",
    "    for word, frequencies in word_frequencies_per_doc.items():    \n",
    "        if word in medical_dict:\n",
    "            medical_dict[word] = frequency\n",
    "    return medical_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load both general and medical dictionaries\n",
    "def initialize_symspell_with_dictionaries(general_dict_path, medical_dict):\n",
    "    # Initialize SymSpell\n",
    "    # Change max_dictionary_edit_distance as suitable for you \n",
    "    # make sure to change it in correct_tokens_with_symspell(sym_spell, tokens) as well\n",
    "    sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "    \n",
    "    # Load the general dictionary\n",
    "    for term, freq in general_dict.items():\n",
    "        sym_spell.create_dictionary_entry(term, freq)\n",
    "\n",
    "    # Add medical terms to the SymSpell dictionary\n",
    "    for term, freq in medical_dict.items():\n",
    "        sym_spell.create_dictionary_entry(term, freq)\n",
    "    \n",
    "    return sym_spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_general_dictionary(file_path):\n",
    "    dictionary = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            term, freq = line.split()  # Split with space\n",
    "            dictionary[term] = int(freq) \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspellpy import SymSpell, Verbosity\n",
    "def correct_tokens_with_symspell(sym_spell, tokens):\n",
    "    corrected_tokens = []\n",
    "    for word in tokens:\n",
    "        # Get suggestions for each word\n",
    "        # Change max_dictionary_edit_distance as suitable for you \n",
    "        # make sure to change it initialize_symspell_with_dictionaries(general_dict_path, medical_dict) as well\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.ALL, max_edit_distance=2)\n",
    "        if suggestions:\n",
    "            # Take the most frequent suggestion\n",
    "            corrected_word = suggestions[0].term\n",
    "        else:\n",
    "            corrected_word = word  # Keep the original word if no suggestion is found\n",
    "        corrected_tokens.append(corrected_word)\n",
    "    \n",
    "    return corrected_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(file_path):\n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    if file_extension == '.json':\n",
    "        return read_json(file_path)\n",
    "    elif file_extension == '.txt':\n",
    "        return read_txt(file_path)\n",
    "    elif file_extension == '.xml':\n",
    "        return read_xml(file_path)\n",
    "    elif file_extension == '.html':\n",
    "        return read_html(file_path)  \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "\n",
    "folder_path = \"C:/Users/yassm/OneDrive/Desktop/Github/Indexing and Search Engine for Medical Research Articles Project/docs\"\n",
    "documents = {}\n",
    "file_name_map = {}\n",
    "index = 0\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        try:\n",
    "            # Use read_document to read based on file type\n",
    "            documents[filename] = read_document(file_path)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "    \n",
    "    file_name_map[filename] = index\n",
    "    index += 1\n",
    "\n",
    "print(\"File Name Map:\")\n",
    "print(file_name_map)\n",
    "print(\"\\nDocuments Content:\")\n",
    "for doc, content in documents.items():\n",
    "    print(doc, \":\", content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace abbreviations in the list of tokens with their full forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviation_mapping = {\n",
    "    \"abg\": \"arterial blood gases\",\n",
    "    \"ace\": \"angiotensin converting enzyme\",\n",
    "    \"acl\": \"anterior cruciate ligament\",\n",
    "    \"adhd\": \"attention deficit hyperactivity disorder\",\n",
    "    \"afib\": \"atrial fibrillation\",\n",
    "    \"aids\": \"acquired immunodeficiency syndrome\",\n",
    "    \"alp\": \"alkaline phosphatase\",\n",
    "    \"als\": \"amyotrophic lateral sclerosis\",\n",
    "    \"alt\": \"alanine aminotransferase\",\n",
    "    \"amd\": \"age related macular degeneration\",\n",
    "    \"ami\": \"acute myocardial infarction\",\n",
    "    \"aodm\": \"adult onset diabetes mellitus\",\n",
    "    \"ast\": \"aspartate aminotransferase\",\n",
    "    \"avm\": \"arteriovenous malformation\",\n",
    "    \"bid\": \"twice a day\",\n",
    "    \"bmi\": \"body mass index\",\n",
    "    \"bp\": \"blood pressure\",\n",
    "    \"bph\": \"benign prostatic hypertrophy\",\n",
    "    \"brca\": \"breast cancer gene\",\n",
    "    \"bun\": \"blood urea nitrogen\",\n",
    "    \"ca\": \"cancer or calcium\",\n",
    "    \"ca-125\": \"cancer antigen 125\",\n",
    "    \"cabg\": \"coronary artery bypass graft\",\n",
    "    \"cad\": \"coronary artery disease\",\n",
    "    \"cat\": \"computerized axial tomography\",\n",
    "    \"cbc\": \"complete blood count\",\n",
    "    \"chd\": \"congenital heart disease\",\n",
    "    \"chf\": \"congestive heart failure\",\n",
    "    \"cmv\": \"cytomegalovirus\",\n",
    "    \"cns\": \"central nervous system\",\n",
    "    \"copd\": \"chronic obstructive pulmonary disease\",\n",
    "    \"cpk\": \"creatine phosphokinase\",\n",
    "    \"cpr\": \"cardiopulmonary resuscitation\",\n",
    "    \"crf\": \"chronic renal failure\",\n",
    "    \"crp\": \"c reactive protein\",\n",
    "    \"csf\": \"cerebrospinal fluid\",\n",
    "    \"cva\": \"cerebrovascular accident\",\n",
    "    \"cxr\": \"chest x ray\",\n",
    "    \"d&c\": \"dilatation and curettage\",\n",
    "    \"djd\": \"degenerative joint disease\",\n",
    "    \"dm\": \"diabetes mellitus\",\n",
    "    \"dtp\": \"diphtheria, tetanus, pertussis\",\n",
    "    \"dvt\": \"deep vein thrombosis\",\n",
    "    \"dx\": \"diagnosis\",\n",
    "    \"ecg\": \"electrocardiogram\",\n",
    "    \"echo\": \"echocardiogram\",\n",
    "    \"eeg\": \"electroencephalogram\",\n",
    "    \"emg\": \"electromyography\",\n",
    "    \"ent\": \"ear, nose and throat\",\n",
    "    \"ercp\": \"endoscopic retrograde cholangiopancreatography\",\n",
    "    \"esr\": \"erythrocyte sedimentation rate\",\n",
    "    \"esrd\": \"end stage renal (kidney) disease\",\n",
    "    \"fsh\": \"follicle stimulating hormone\",\n",
    "    \"gerd\": \"gastroesophageal reflux disease\",\n",
    "    \"gi\": \"gastrointestinal\",\n",
    "    \"gfr\": \"glomerular filtration rate\",\n",
    "    \"gu\": \"genitourinary\",\n",
    "    \"hav\": \"hepatitis a virus\",\n",
    "    \"hbv\": \"hepatitis b virus\",\n",
    "    \"hct\": \"hematocrit\",\n",
    "    \"hcv\": \"hepatitis c virus\",\n",
    "    \"hdl\": \"high density lipoprotein\",\n",
    "    \"hgb\": \"hemoglobin\",\n",
    "    \"hiv\": \"human immunodeficiency virus\",\n",
    "    \"hpv\": \"human papilloma virus\",\n",
    "    \"hrt\": \"hormone replacement therapy\",\n",
    "    \"htn\": \"hypertension\",\n",
    "    \"ibd\": \"inflammatory bowel disease\",\n",
    "    \"ibs\": \"irritable bowel syndrome\",\n",
    "    \"icd\": \"implantable cardioverter defibrillator\",\n",
    "    \"icu\": \"intensive care unit\",\n",
    "    \"iddm\": \"insulin dependent diabetes mellitus\",\n",
    "    \"im\": \"intramuscular\",\n",
    "    \"iud\": \"intrauterine device\",\n",
    "    \"iv\": \"intravenous\",\n",
    "    \"ivp\": \"intravenous pyelogram\",\n",
    "    \"ldl\": \"low density lipoprotein\",\n",
    "    \"lft\": \"liver function tests\",\n",
    "    \"mi\": \"myocardial infarction\",\n",
    "    \"mmr\": \"measles, mumps, and rubella\",\n",
    "    \"mri\": \"magnetic resonance imaging\",\n",
    "    \"mrsa\": \"methicillin resistant staphylococcus aureus\",\n",
    "    \"ms\": \"multiple sclerosis\",\n",
    "    \"ng\": \"nasogastric\",\n",
    "    \"niddm\": \"non insulin dependent diabetes mellitus\",\n",
    "    \"nkda\": \"no known drug allergies\",\n",
    "    \"nsaid\": \"non steroidal anti inflammatory drug\",\n",
    "    \"ocd\": \"obsessive compulsive disorder\",\n",
    "    \"pad\": \"peripheral arterial disease\",\n",
    "    \"pap\": \"papanicolau\",\n",
    "    \"pat\": \"paroxysmal atrial tachycardia\",\n",
    "    \"pet\": \"positron emission tomography\",\n",
    "    \"pft\": \"pulmonary function test\",\n",
    "    \"pid\": \"pelvic inflammatory disease\",\n",
    "    \"pms\": \"premenstrual syndrome\",\n",
    "    \"ppd\": \"purified protein derivative\",\n",
    "    \"prn\": \"as needed\",\n",
    "    \"psa\": \"prostate specific antigen\",\n",
    "    \"pt\": \"prothrombin time\",\n",
    "    \"pth\": \"parathyroid hormone\",\n",
    "    \"ptsd\": \"post traumatic stress syndrome\",\n",
    "    \"ptt\": \"partial thromboplastin time\",\n",
    "    \"pud\": \"peptic ulcer disease\",\n",
    "    \"pvc\": \"premature ventricular contraction\",\n",
    "    \"qid\": \"four times a day\",\n",
    "    \"ra\": \"rheumatoid arthritis\",\n",
    "    \"rbc\": \"red blood cell\",\n",
    "    \"rsv\": \"respiratory syncytial virus\",\n",
    "    \"rx\": \"prescription\",\n",
    "    \"sad\": \"seasonal affective disorder\",\n",
    "    \"sids\": \"sudden infant death syndrome\",\n",
    "    \"sle\": \"systemic lupus erythematosus\",\n",
    "    \"sob\": \"shortness of breath\",\n",
    "    \"std\": \"sexually transmitted disease\",\n",
    "    \"t3\": \"triiodothyronine\",\n",
    "    \"t4\": \"thyroxine\",\n",
    "    \"tb\": \"tuberculosis\",\n",
    "    \"tah\": \"total abdominal hysterectomy\",\n",
    "    \"tia\": \"transient ischemic attack\",\n",
    "    \"tibc\": \"total iron binding capacity\",\n",
    "    \"tid\": \"three times a day\",\n",
    "    \"tmj\": \"temporomandibular joint\",\n",
    "    \"torch\": \"infections that may cause birth defects\",\n",
    "    \"tsh\": \"thyroid stimulating hormone\",\n",
    "    \"turp\": \"transurethral resection of prostate gland\",\n",
    "    \"uri\": \"upper respiratory infection\",\n",
    "    \"uti\": \"urinary tract infection\",\n",
    "    \"xrt\": \"radiotherapy\",\n",
    "    \"wbc\": \"white blood cell\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_abbreviations(tokens):\n",
    "    replaced_tokens = []\n",
    "    for token in tokens:\n",
    "        full_form = abbreviation_mapping.get(token, token)  # Get full form or original token\n",
    "        replaced_tokens.append(full_form)\n",
    "    return replaced_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_punctuation(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #add stopwords to stop_words here\n",
    "    stop_words.update({\n",
    "        \"et\", \"cetera\", \"xml\", \"json\", \"schema\", \"attribute\", \"element\", \n",
    "        \"document\", \"data\", \"version\", \"type\", \"id\", \"name\", \"value\", \n",
    "        \"namespace\", \"lang\", \"date\", \"ref\", \"content\", \"tag\", \"api\", \"schema\", \n",
    "        \"url\", \"http\", \"https\", \"base\", \"file\", \"result\", \"request\", \"response\", \n",
    "        \"title\", \"cell\", \"text\", \"body\", \"value\", \"image\", \"reference\", \"record\", \n",
    "        \"metadata\", \"definition\", \"information\", \"description\", \"details\", \"model\",\n",
    "        \"category\", \"item\", \"reference\", \"data\", \"type\", \"category\", \"link\", \"method\", \"yes\", \"no\", \n",
    "        \"the\", \"and\", \"or\", \"not\", \"this\", \"that\", \"it\", \"a\", \"an\", \"of\", \"in\", \"for\", \n",
    "        \"to\", \"on\", \"at\", \"by\", \"with\", \"as\", \"etc\", \"etcetera\", \"thus\", \"here\", \"there\",\n",
    "        \"wiki\", \"author\", \"field\", \"main\", \"section\", \"article\", \"h1\", \"h2\", \"h3\", \"h4\", \n",
    "        \"h5\", \"h6\", \"p\", \"br\", \"a\", \"img\", \"ul\", \"li\", \"ol\", \"strong\", \"em\", \"meta\", \"link\", \n",
    "        \"script\", \"style\", \"head\", \"footer\", \"html\", \"svg\", \"abstract\", \"conclusion\"\n",
    "    })\n",
    "    punctuation = set(string.punctuation)\n",
    "    punctuation.update({'\\n', '\\t', '\\r', '', '-', '--', '---'})\n",
    "    if all(isinstance(i, list) for i in tokens):  # Check if it's a list of lists\n",
    "        tokens = [word for sublist in tokens for word in sublist]  # Flatten the list\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words and word not in punctuation and not word.isdigit()]\n",
    "    return cleaned_tokens\n",
    "\n",
    "\n",
    "# Example:\n",
    "tokens_example = [\n",
    "    \"This\", \"is\", \"an\", \"example\", \"sentence\", \"-\", \"with\", \"punctuation\", \"and\", \"stopwords\", \".\", \n",
    "    \"Also\", \"contains\", \"html\", \"tags\", \"like\", \"p\", \"and\", \"h1\", \"as\", \"well\", \"as\", \"a\", \"date\", \"2024\", \n",
    "    \"schema\", \"type\", \"and\", \"attribute\", \"--\", \"example-field\"\n",
    "]\n",
    "cleaned_tokens_example = remove_stopwords_punctuation(tokens_example)\n",
    "print(cleaned_tokens_example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization using spaCy for better performance and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    # Join tokens into a single string for spaCy processing\n",
    "    text = \" \".join(tokens)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatize each token and return as a list of lemmatized tokens\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "tokens_example = [\"running\", \"jumps\", \"better\", \"children\", \"dogs\", \"geese\"]\n",
    "\n",
    "# Call the lemmatize_tokens function\n",
    "lemmatized_tokens_example = lemmatize_tokens(tokens_example)\n",
    "\n",
    "# Print original tokens and their lemmatized forms\n",
    "print(\"Original tokens: \", tokens_example)\n",
    "print(\"Lemmatized tokens: \", lemmatized_tokens_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_dict = load_general_dictionary(\"frequency_dictionary_en_30_000.txt\")\n",
    "sym_spell = initialize_symspell_with_dictionaries(general_dict, medical_terms_dict)\n",
    "tokens_per_doc = {}\n",
    "tokens_before_removal = {}\n",
    "word_frequencies_per_doc = {}\n",
    "tokens_after_tokenization = {}  \n",
    "tokens_after_lemmatization_and_cleaning = {}  \n",
    "token_count_after_tokenization = 0\n",
    "token_count_after_lemmatization_and_cleaning = 0\n",
    "for filename, content in documents.items():\n",
    "    doc_id = file_name_map.get(filename)\n",
    "    extracted_content = extract_text_with_keys(content)\n",
    "    expanded_content = expand_punctuated_abbreviations(extracted_content)\n",
    "    tokens = tokenize(expanded_content)\n",
    "    tokens_after_tokenization[filename] = tokens\n",
    "    token_count_after_tokenization += len(tokens)\n",
    "    word_frequencies_per_doc[filename] = dict(calculate_word_frequency(tokens))  \n",
    "    medical_terms_dict = update_medical_terms_with_real_frequencies(medical_terms_dict, word_frequencies_per_doc[filename])\n",
    "    sym_spell = initialize_symspell_with_dictionaries(general_dict, medical_terms_dict)\n",
    "    corrected_tokens = correct_tokens_with_symspell(sym_spell, tokens)\n",
    "    replaced_tokens = replace_abbreviations(corrected_tokens)\n",
    "    cleaned_tokens = remove_stopwords_punctuation(replaced_tokens)\n",
    "    lemmatized_tokens = lemmatize_tokens(cleaned_tokens)\n",
    "    lemmatized_cleaned_tokens = remove_stopwords_punctuation(lemmatized_tokens)\n",
    "    tokens_per_doc[doc_id] = lemmatized_cleaned_tokens\n",
    "    tokens_after_lemmatization_and_cleaning[filename] = lemmatized_cleaned_tokens\n",
    "    token_count_after_lemmatization_and_cleaning += len(lemmatized_cleaned_tokens)\n",
    "    #print(f\"cleaned tokens for '{filename}':\", lemmatized_tokens)\n",
    "for doc_id, content in tokens_per_doc.items():\n",
    "    print(f\"Tokens for document {doc_id}:\", content)\n",
    "#print(tokens_before_removal)\n",
    "#print(word_frequencies_per_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length of tokens before and after text preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"  Tokens after tokenization: {token_count_after_tokenization}\")\n",
    "print(f\"  Tokens after lemmatization and cleaning: {token_count_after_lemmatization_and_cleaning}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of correct_tokens_with_symspell\n",
    "tokens = [\"hypertensiom\", \"diagnosus\", \"hrpertension\", \"thys\", \"teh\", \"hyprtension\", \"mai\", \"cmments\", \"csmment\", \"Ths\", \"exmple\", \"txt\", \"sme\", \"spelng\", \"errrs\"]\n",
    "corrected_tokens = correct_tokens_with_symspell(sym_spell, tokens)\n",
    "\n",
    "print(\"Original tokens:\", tokens)\n",
    "print(\"Corrected tokens:\", corrected_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverted index with TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index_with_tf(tokens_per_doc):\n",
    "    inverted_index = {}\n",
    "    for doc_id, tokens in tokens_per_doc.items():\n",
    "        for term in tokens:\n",
    "            if term not in inverted_index:\n",
    "                inverted_index[term] = {doc_id: 1}\n",
    "            else:\n",
    "                if doc_id not in inverted_index[term]:\n",
    "                    inverted_index[term][doc_id] = 1\n",
    "                else:\n",
    "                    inverted_index[term][doc_id] += 1\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverted index with positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index_with_positions(tokens_per_doc):\n",
    "    inverted_index = {}\n",
    "    for doc_id, tokens in tokens_per_doc.items():\n",
    "        for position, term in enumerate(tokens):\n",
    "            if term not in inverted_index:\n",
    "                inverted_index[term] = {}\n",
    "            if doc_id not in inverted_index[term]:\n",
    "                inverted_index[term][doc_id] = []\n",
    "            inverted_index[term][doc_id].append(position)\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverted index with DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index_with_df(tokens_per_doc):\n",
    "    inverted_index = {}\n",
    "    for doc_id, tokens in tokens_per_doc.items():\n",
    "        for word in tokens:\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = {\"DF\": 1, \"PL\": {doc_id: 1}}\n",
    "            else:\n",
    "                if doc_id not in inverted_index[word][\"PL\"]:\n",
    "                    inverted_index[word][\"PL\"][doc_id] = 1  \n",
    "                    inverted_index[word][\"DF\"] += 1         \n",
    "                else:\n",
    "                    inverted_index[word][\"PL\"][doc_id] += 1  \n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call the inverted index functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index_tf = create_inverted_index_with_tf(tokens_per_doc)\n",
    "print(\"Inverted Index with Term Frequency:\", inverted_index_tf)\n",
    "\n",
    "inverted_index_positions = create_inverted_index_with_positions(tokens_per_doc)\n",
    "print(\"Inverted Index with Term Positions:\", inverted_index_positions)\n",
    "\n",
    "inverted_index_df = create_inverted_index_with_df(tokens_per_doc)\n",
    "print(\"Inverted Index with Document Frequency and Posting List:\", inverted_index_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency-Inverse Document Frequency (TF-IDF) Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words that cannot be indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def create_tf_idf_index(inverted_index_df, total_docs):\n",
    "    tf_idf_index = {}\n",
    "    zero_weight_terms = set()\n",
    "    for term, data in inverted_index_df.items():\n",
    "        doc_frequency = data[\"DF\"]\n",
    "        idf = math.log(total_docs / doc_frequency)\n",
    "        tf_idf_index[term] = {}\n",
    "        \n",
    "        for doc_id, term_frequency in data[\"PL\"].items():\n",
    "            tf_idf_weight = term_frequency * idf\n",
    "            tf_idf_index[term][doc_id] = tf_idf_weight\n",
    "            if tf_idf_weight == 0:\n",
    "                zero_weight_terms.add(term)\n",
    "    \n",
    "    return tf_idf_index, zero_weight_terms\n",
    "total_docs = len(file_name_map)\n",
    "tf_idf_index, zero_weight_terms = create_tf_idf_index(inverted_index_df, total_docs)\n",
    "print(\"TF-IDF Index:\")\n",
    "print(tf_idf_index)\n",
    "print(\"\\nWords with weight = 0 (Cannot be indexed):\")\n",
    "print(zero_weight_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure terms by document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_terms_by_document(tf_idf_index):\n",
    "    terms_by_doc = {}  \n",
    "    for term, doc_weights in tf_idf_index.items():\n",
    "        for doc_id, weight in doc_weights.items():\n",
    "            if doc_id not in terms_by_doc:\n",
    "                terms_by_doc[doc_id] = []\n",
    "            terms_by_doc[doc_id].append((term, weight)) \n",
    "    return terms_by_doc\n",
    "\n",
    "terms_by_doc = structure_terms_by_document(tf_idf_index)\n",
    "for doc_id, content in terms_by_doc.items():\n",
    "    print(doc_id, \": \", content, \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve top terms for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_weighted_terms(terms_by_doc, top_n=5):\n",
    "    top_terms_per_doc = {}\n",
    "    for doc_id, terms in terms_by_doc.items():\n",
    "        top_terms_per_doc[doc_id] = sorted(terms, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    return top_terms_per_doc\n",
    "\n",
    "top_weighted_terms_per_doc = get_top_weighted_terms(terms_by_doc, top_n=5)\n",
    "for doc_id, terms in top_weighted_terms_per_doc.items():\n",
    "    print(f\"Top terms for document {doc_id}: {terms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>QUERY</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_query_interface():\n",
    "    user_query = input(\"Enter your search query: \")\n",
    "\n",
    "    tokens = tokenize(user_query)  \n",
    "    \n",
    "    corrected_tokens = correct_tokens_with_symspell(sym_spell, tokens)\n",
    "    \n",
    "    replaced_tokens = replace_abbreviations(corrected_tokens)\n",
    "    \n",
    "    cleaned_tokens = remove_stopwords_punctuation(replaced_tokens)\n",
    "    \n",
    "    lemmatized_tokens = lemmatize_tokens(cleaned_tokens)\n",
    "    \n",
    "    lemmatized_cleaned_tokens = remove_stopwords_punctuation(lemmatized_tokens)\n",
    "    \n",
    "    print(f\"Cleaned tokens for query:\", lemmatized_cleaned_tokens)\n",
    "    return lemmatized_cleaned_tokens\n",
    "query_terms = user_query_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relevance(tf_idf_index, query_terms, doc_id):\n",
    "    relevance_score = 0\n",
    "    for term in query_terms:\n",
    "        if term in tf_idf_index and doc_id in tf_idf_index[term]:\n",
    "            relevance_score += tf_idf_index[term][doc_id] \n",
    "    return relevance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents_by_relevance(tf_idf_index, query_terms, documents, top_n=3):\n",
    "    relevance_scores = {} \n",
    "    for doc_id in documents.keys():\n",
    "        relevance_scores[doc_id] = calculate_relevance(tf_idf_index, query_terms, doc_id)\n",
    "        \n",
    "    # Sort the documents by relevance score (from highest to lowest)\n",
    "    ranked_documents = sorted(relevance_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_documents = ranked_documents[:top_n] \n",
    "    return top_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_relevant_documents = rank_documents_by_relevance(tf_idf_index, query_terms, tokens_per_doc, top_n=3)\n",
    "print(\"Top 3 most relevant documents:\")\n",
    "reversed_file_name_map = {v: k for k, v in file_name_map.items()}\n",
    "for doc_id, score in top_3_relevant_documents:\n",
    "    document_name = reversed_file_name_map.get(doc_id)\n",
    "    print(f\"Document: {doc_id} (Name: {document_name}), Relevance Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "def generate_two_grams_per_document(tokens_per_doc):\n",
    "    two_grams_per_doc = {}  # Initialize a dictionary for 2-grams per document\n",
    "    for doc_id, tokens in tokens_per_doc.items():\n",
    "        two_grams = list(ngrams(tokens, 2))  # Generate 2-grams for each document\n",
    "        two_grams_per_doc[doc_id] = two_grams  # Store the list of 2-grams in the dictionary\n",
    "    return two_grams_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_grams_per_doc = generate_two_grams_per_document(tokens_per_doc)\n",
    "print(two_grams_per_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inverted index for 2-grams\n",
    "inverted_index_two_grams = create_inverted_index_with_df(two_grams_per_doc)\n",
    "print(\"Inverted Index for 2-grams:\")\n",
    "print(inverted_index_two_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_index_two_grams, zero_weight_terms_2grams = create_tf_idf_index(inverted_index_two_grams, total_docs)\n",
    "print(\"TF-IDF Index for 2-grams:\")\n",
    "print(tf_idf_index_two_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigrams(tokens):\n",
    "    if len(tokens) < 2:\n",
    "        return []  # Return an empty list if there is only one token\n",
    "    return [tuple(tokens[i:i+2]) for i in range(len(tokens)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_terms_2grams = generate_bigrams(query_terms)\n",
    "print(query_terms_2grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relevance_2grams(tf_idf_index_two_grams, query_terms_2grams, doc_id):\n",
    "    relevance_score = 0\n",
    "    for term in query_terms_2grams:\n",
    "        if term in tf_idf_index_two_grams and doc_id in tf_idf_index_two_grams[term]:\n",
    "            relevance_score += tf_idf_index_two_grams[term][doc_id]\n",
    "    return relevance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents_by_relevance_2grams(tf_idf_index_two_grams, query_terms_2grams, documents, top_n=3):\n",
    "    relevance_scores = {} \n",
    "    for doc_id in documents.keys():\n",
    "        relevance_scores[doc_id] = calculate_relevance_2grams(tf_idf_index_two_grams, query_terms_2grams, doc_id)\n",
    "    # Sort the documents by relevance score (from highest to lowest)\n",
    "    ranked_documents = sorted(relevance_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the top N documents based on relevance scores\n",
    "    top_documents = ranked_documents[:top_n] \n",
    "    \n",
    "    return top_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_relevant_documents_2grams = rank_documents_by_relevance_2grams(tf_idf_index_two_grams, query_terms_2grams, two_grams_per_doc, top_n=3)\n",
    "print(\"Top 3 most relevant documents:\")\n",
    "\n",
    "# Loop through the top 3 relevant documents\n",
    "for doc_id, score in top_3_relevant_documents_2grams:\n",
    "    document_name = reversed_file_name_map.get(doc_id)\n",
    "    print(f\"Document: {doc_id} (Name: {document_name}), Relevance Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Can be used later</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ngrams function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams_per_document(tokens_per_doc, n_values=(2, 3)):\n",
    "    n_grams_per_doc = {n: {} for n in n_values}  # Initialize dictionaries for each n value\n",
    "    for filename, tokens in tokens_per_doc.items():\n",
    "        for n in n_values:\n",
    "            n_grams = ngrams(tokens, n)\n",
    "            n_grams_per_doc[n][filename] = list(n_grams)  \n",
    "    return n_grams_per_doc\n",
    "n_values = (2, 3) \n",
    "n_grams_per_doc = generate_ngrams_per_document(tokens_per_doc, n_values)\n",
    "\n",
    "\n",
    "#Example:\n",
    "tokens_per_doc_example = {\n",
    "    \"doc1\": [\"this\", \"is\", \"a\", \"sample\", \"document\"],\n",
    "    \"doc2\": [\"another\", \"document\", \"with\", \"different\", \"words\"]\n",
    "}\n",
    "n_values_example = (2, 3, 4)\n",
    "n_grams_per_doc_example = generate_ngrams_per_document(tokens_per_doc_example, n_values_example)\n",
    "\n",
    "for n, docs in n_grams_per_doc_example.items():\n",
    "    print(f\"\\n{n}-grams per document:\")\n",
    "    for doc, n_grams in docs.items():\n",
    "        print(f\"{doc}: {n_grams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "def correct_text(text):\n",
    "    corrected_text = str(TextBlob(text).correct())\n",
    "    return corrected_text\n",
    "\n",
    "# Example:\n",
    "input_text = \"hrpertension\"\n",
    "corrected_output = correct_text(input_text)\n",
    "print(\"Original:\", input_text)\n",
    "print(\"Corrected:\", corrected_output)\n",
    "# =======================================> not very practical (will not be used in this context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accent removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accents(text):\n",
    "    normalized_text = unicodedata.normalize('NFKD', text) #Normalization Form KD, where \"KD\" refers to Compatibility Decomposition\n",
    "    without_accents = ''.join(char for char in normalized_text if not unicodedata.combining(char))\n",
    "    return without_accents\n",
    "\n",
    "# Example:\n",
    "french_text = \"Élévation de température et maux de tête.\"\n",
    "cleaned_text = remove_accents(french_text)\n",
    "print(\"Text without accents:\", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_terms(terms, n):\n",
    "    truncated_terms = [term[:n] for term in terms]\n",
    "    return truncated_terms\n",
    "\n",
    "# Example:\n",
    "terms = ['medication', 'medical', 'methodology', 'medicine', 'mechanism']\n",
    "\n",
    "n = 3\n",
    "truncated_terms = truncate_terms(terms, n)\n",
    "\n",
    "print(\"Original Terms:\", terms)\n",
    "print(\"Truncated Terms (to 3 characters):\", truncated_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "\n",
    "def normalize_date(date_string, output_format=\"%Y-%m-%d\"):\n",
    "    try:\n",
    "        # Use dateutil.parser to parse the date string\n",
    "        parsed_date = parser.parse(date_string)\n",
    "        \n",
    "        # Return the date in the specified output format\n",
    "        return parsed_date.strftime(output_format)\n",
    "    \n",
    "    except ValueError:\n",
    "        print(f\"Error: Unable to parse date '{date_string}'\")\n",
    "        return None\n",
    "\n",
    "# Example:\n",
    "dates = [\n",
    "    \"2023-03-15\", \"15th March, 2023\", \"March 15, 2023\", \"03/15/2023\", \"2023/03/15\", \"2023-03-15 14:30:00\"\n",
    "]\n",
    "\n",
    "normalized_dates = [normalize_date(date) for date in dates]\n",
    "print(\"Normalized Dates:\")\n",
    "for original, normalized in zip(dates, normalized_dates):\n",
    "    print(f\"Original: {original} -> Normalized: {normalized}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
